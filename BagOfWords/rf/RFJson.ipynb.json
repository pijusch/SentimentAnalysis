{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7587d33544fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpunctuation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mos\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#Open file in read-only and extract content into variable 'content'\n",
    "def loadFile(filename):\n",
    "    openFile = open(filename, 'r')\n",
    "    content = openFile.read()\n",
    "    openFile.close()\n",
    "    return content\n",
    "\n",
    "#Tokenize file\n",
    "def tokenizeFile(filename):\n",
    "    tokens = filename.split()                                   #remove whitespace\n",
    "    tokens = [x.strip(string.punctuation) for x in tokens]           #remove punctuation\n",
    "    tokens = [word for word in tokens if word.isalpha()]        #remove none alphabetic words\n",
    "    stopWords = set(stopwords.words('english'))                 #remove stop words\n",
    "    tokens = [word for word in tokens if not word in stopWords]\n",
    "    tokens = [word for word in tokens if len(word) > 1]         #remove 1-letter tokens\n",
    "    return tokens\n",
    "\n",
    "#Convert tokens to single strings for easier encoding\n",
    "def fileToLine(filename, vocab):\n",
    "    content = loadFile(filename)\n",
    "    tokens = tokenizeFile(content)\n",
    "    tokens = [word for word in tokens if word in vocab]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "#Load all reviews and start mapping words to counter\n",
    "def loadReviews(directory, vocab, is_train):\n",
    "    lines = list()\n",
    "    for filename in listdir(directory):\n",
    "        if filename.startswith('cv9') and is_train:\n",
    "            continue\n",
    "        if not filename.startswith('cv9') and not is_train:\n",
    "            continue\n",
    "        path = directory + '/' + filename\n",
    "        line = fileToLine(path, vocab)\n",
    "        lines.append(line)\n",
    "    return lines\n",
    "\n",
    "#Predict reviews based on MLP network\n",
    "def predictReview(review, vocab, tokenizer, model):\n",
    "\n",
    "    #Split review into words and filter based on current vocab\n",
    "    tokens = tokenizeFile(review)\n",
    "    tokens = [word for word in tokens if word in vocab]\n",
    "    lines = ' '.join(tokens)\n",
    "    encode = tokenizer.texts_to_matrix([lines], mode='freq')\n",
    "\n",
    "    #Predict review: 0 if positive, 1 if negative\n",
    "    y = model.predict(encode, verbose=0)\n",
    "    return round(y[0,0])\n",
    "\n",
    "\n",
    "### Main ###\n",
    "vocabFile = 'vocab.txt'\n",
    "vocab = loadFile(vocabFile)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "\n",
    "print(\"Loading data sets...\")\n",
    "\n",
    "#Training set\n",
    "pos_reviews = loadReviews('data/pos', vocab, True)\n",
    "neg_reviews = loadReviews('data/neg', vocab, True)\n",
    "tokenizer = Tokenizer()\n",
    "total = pos_reviews+neg_reviews\n",
    "tokenizer.fit_on_texts(total)\n",
    "x_train = tokenizer.texts_to_matrix(total, mode='freq')\n",
    "y_train = array([0 for _ in range(900)] + [1 for _ in range(900)])\n",
    "\n",
    "#Testing Set\n",
    "pos_reviews = loadReviews('data/pos', vocab, False)\n",
    "neg_reviews = loadReviews('data/neg', vocab, False)\n",
    "total = pos_reviews+neg_reviews\n",
    "x_test = tokenizer.texts_to_matrix(total, mode='freq')\n",
    "y_test = array([0 for _ in range(100)] + [1 for _ in range(100)])\n",
    "\n",
    "#Train Random Forest\n",
    "RForest = RandomForestClassifier(n_estimators=15, criterion=\"gini\", max_depth=None, min_samples_split=2, max_features=\"sqrt\")\n",
    "RForest.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions on unseen test data\n",
    "predictions = RForest.predict(x_test)\n",
    "print(\"Accuracy: {}%\".format(RForest.score(x_test, y_test) * 100 ))\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
