{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data sets...\n",
      "Training the classifier...\n",
      "[LibLinear]Training Complete!\n",
      "Accuracy: 88.0%\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "#Open file in read-only and extract content into variable 'content'\n",
    "def loadFile(filename):\n",
    "    openFile = open(filename, 'r')\n",
    "    content = openFile.read()\n",
    "    openFile.close()\n",
    "    return content\n",
    "\n",
    "#Tokenize file\n",
    "def tokenizeFile(filename):\n",
    "    tokens = filename.split()                                   #remove whitespace\n",
    "    tokens = [x.strip(string.punctuation) for x in tokens]           #remove punctuation\n",
    "    tokens = [word for word in tokens if word.isalpha()]        #remove none alphabetic words\n",
    "    stopWords = set(stopwords.words('english'))                 #remove stop words\n",
    "    tokens = [word for word in tokens if not word in stopWords]\n",
    "    tokens = [word for word in tokens if len(word) > 1]         #remove 1-letter tokens\n",
    "    return tokens\n",
    "\n",
    "#Convert tokens to single strings for easier encoding\n",
    "def fileToLine(filename, vocab):\n",
    "    content = loadFile(filename)\n",
    "    tokens = tokenizeFile(content)\n",
    "    tokens = [word for word in tokens if word in vocab]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "#Load all reviews and start mapping words to counter\n",
    "def loadReviews(directory, vocab, is_train):\n",
    "    lines = list()\n",
    "    for filename in listdir(directory):\n",
    "        if filename.startswith('cv9') and is_train:\n",
    "            continue\n",
    "        if not filename.startswith('cv9') and not is_train:\n",
    "            continue\n",
    "        path = directory + '/' + filename\n",
    "        line = fileToLine(path, vocab)\n",
    "        lines.append(line)\n",
    "    return lines\n",
    "\n",
    "#Predict reviews based on MLP network\n",
    "def predictReview(review, vocab, tokenizer, model):\n",
    "\n",
    "    #Split review into words and filter based on current vocab\n",
    "    tokens = tokenizeFile(review)\n",
    "    tokens = [word for word in tokens if word in vocab]\n",
    "    lines = ' '.join(tokens)\n",
    "    encode = tokenizer.texts_to_matrix([lines], mode='freq')\n",
    "\n",
    "    #Predict review: 0 if positive, 1 if negative\n",
    "    y = model.predict(encode, verbose=0)\n",
    "    return round(y[0,0])\n",
    "\n",
    "\n",
    "### Main ###\n",
    "vocabFile = 'vocab.txt'\n",
    "vocab = loadFile(vocabFile)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "\n",
    "print(\"Loading data sets...\")\n",
    "\n",
    "#Training set\n",
    "pos_reviews = loadReviews('data/pos', vocab, True)\n",
    "neg_reviews = loadReviews('data/neg', vocab, True)\n",
    "tokenizer = Tokenizer()\n",
    "total = pos_reviews+neg_reviews\n",
    "tokenizer.fit_on_texts(total)\n",
    "x_train = tokenizer.texts_to_matrix(total, mode='freq')\n",
    "y_train = array([0 for _ in range(900)] + [1 for _ in range(900)])\n",
    "\n",
    "#Testing Set\n",
    "pos_reviews = loadReviews('data/pos', vocab, False)\n",
    "neg_reviews = loadReviews('data/neg', vocab, False)\n",
    "total = pos_reviews+neg_reviews\n",
    "x_test = tokenizer.texts_to_matrix(total, mode='freq')\n",
    "y_test = array([0 for _ in range(100)] + [1 for _ in range(100)])\n",
    "\n",
    "\n",
    "#SVM:\n",
    "clf = svm.LinearSVC(C=40, class_weight=None, dual=True, fit_intercept=True,\n",
    "     intercept_scaling=1, loss='squared_hinge', max_iter=10000,\n",
    "     multi_class='crammer_singer', penalty='l2', random_state=None, tol=0.001,\n",
    "     verbose=1)\n",
    "print(\"Training the classifier...\");\n",
    "# Train classifier \n",
    "clf.fit(x_train, y_train)\n",
    "print(\"Training Complete!\");\n",
    "\n",
    "# Make predictions on unseen test data\n",
    "clf_predictions = clf.predict(x_test)\n",
    "print(\"Accuracy: {}%\".format(clf.score(x_test, y_test) * 100 ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
